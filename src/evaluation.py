"""Model evaluation and business metrics for equipment price prediction."""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from typing import Dict, List, Tuple, Optional
import warnings
from pathlib import Path

warnings.filterwarnings('ignore')


def create_sophisticated_baselines(df: pd.DataFrame, target_col: str = 'sales_price',
                                 product_group_col: str = None, date_col: str = 'sales_date') -> Dict[str, np.ndarray]:
    """Generate comprehensive baseline predictions for model comparison.
    
    Creates multiple sophisticated baselines that demonstrate business acumen
    and provide meaningful performance context for ML models.
    
    Args:
        df: DataFrame with features and target
        target_col: Name of target column
        product_group_col: Name of product group column for group-specific baselines
        date_col: Name of date column for temporal baselines
        
    Returns:
        Dictionary mapping baseline names to prediction arrays
    """
    n_samples = len(df)
    baselines = {}
    
    # 1. Global Median Baseline
    global_median = df[target_col].median()
    baselines['global_median'] = np.full(n_samples, global_median)
    
    # 2. Product Group Median (if available)
    if product_group_col and product_group_col in df.columns:
        group_medians = df.groupby(product_group_col)[target_col].median()
        # Handle missing groups with global median fallback
        baselines['group_median'] = df[product_group_col].map(group_medians).fillna(global_median).values
    else:
        baselines['group_median'] = baselines['global_median'].copy()
    
    # 3. Temporal Trend Baseline
    if date_col in df.columns and pd.api.types.is_datetime64_any_dtype(df[date_col]):
        # Linear trend over time
        df_temp = df.copy()
        df_temp['days_since_start'] = (df_temp[date_col] - df_temp[date_col].min()).dt.days
        
        # Fit simple linear regression: price ~ time
        from sklearn.linear_model import LinearRegression
        lr = LinearRegression()
        X_time = df_temp[['days_since_start']]
        lr.fit(X_time, df_temp[target_col])
        baselines['temporal_trend'] = lr.predict(X_time)
        
        # 4. Seasonal Baseline (monthly medians)
        monthly_medians = df_temp.groupby(df_temp[date_col].dt.month)[target_col].median()
        baselines['seasonal_monthly'] = df_temp[date_col].dt.month.map(monthly_medians).fillna(global_median).values
    else:
        baselines['temporal_trend'] = baselines['global_median'].copy()
        baselines['seasonal_monthly'] = baselines['global_median'].copy()
    
    # 5. Age-Adjusted Baseline (if age information available)
    age_cols = ['age_at_sale', 'equipment_age', 'age']
    age_col = None
    for col in age_cols:
        if col in df.columns:
            age_col = col
            break
    
    if age_col:
        # Age-based price adjustment using depreciation curve
        age_bins = pd.cut(df[age_col], bins=10, labels=False)
        age_medians = df.groupby(age_bins)[target_col].median()
        baselines['age_adjusted'] = age_bins.map(age_medians).fillna(global_median).values
    else:
        baselines['age_adjusted'] = baselines['global_median'].copy()
    
    # 6. Combined Heuristic (Group + Age + Season)
    if product_group_col and age_col and date_col in df.columns:
        # Multi-factor baseline
        combined_pred = []
        for _, row in df.iterrows():
            base_price = baselines['group_median'][_] if not pd.isna(baselines['group_median'][_]) else global_median
            
            # Age adjustment factor (newer = higher price)
            age = row[age_col] if pd.notna(row[age_col]) else df[age_col].median()
            age_factor = max(0.5, 1.0 - (age / 30) * 0.5)  # Depreciate over 30 years
            
            # Seasonal adjustment (simple)
            if pd.api.types.is_datetime64_any_dtype(df[date_col]):
                month = row[date_col].month
                seasonal_factor = 1.0 + 0.1 * np.sin(2 * np.pi * (month - 3) / 12)  # Peak in spring
            else:
                seasonal_factor = 1.0
            
            adjusted_price = base_price * age_factor * seasonal_factor
            combined_pred.append(adjusted_price)
        
        baselines['combined_heuristic'] = np.array(combined_pred)
    else:
        baselines['combined_heuristic'] = baselines['group_median'].copy()
    
    # 7. Market Trend Baseline (recent performance)
    if date_col in df.columns and pd.api.types.is_datetime64_any_dtype(df[date_col]):
        df_sorted = df.sort_values(date_col)
        recent_period = df_sorted[date_col] >= (df_sorted[date_col].max() - pd.Timedelta(days=365))
        recent_median = df_sorted.loc[recent_period, target_col].median()
        
        # Trend adjustment based on recent vs. historical median
        historical_median = df_sorted.loc[~recent_period, target_col].median()
        trend_factor = recent_median / historical_median if historical_median > 0 else 1.0
        
        baselines['market_trend'] = baselines['group_median'] * trend_factor
    else:
        baselines['market_trend'] = baselines['group_median'].copy()
    
    # Ensure all baselines are positive (equipment prices should be > 0)
    for name in baselines:
        baselines[name] = np.maximum(baselines[name], 1000)  # Minimum $1000
    
    print(f"\nGenerated {len(baselines)} sophisticated baselines:")
    for name, preds in baselines.items():
        print(f"   {name}: median=${np.median(preds):,.0f}, std=${np.std(preds):,.0f}")
    
    return baselines


def evaluate_against_baselines(y_true: np.ndarray, y_pred: np.ndarray, 
                             baselines: Dict[str, np.ndarray]) -> Dict[str, any]:
    """Compare model performance against sophisticated baselines.
    
    Args:
        y_true: True target values
        y_pred: Model predictions
        baselines: Dictionary of baseline predictions
        
    Returns:
        Dictionary with baseline comparison results
    """
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    
    # Calculate model performance
    model_rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    model_mae = mean_absolute_error(y_true, y_pred)
    model_r2 = r2_score(y_true, y_pred)
    
    # Calculate baseline performance
    baseline_performance = {}
    for name, baseline_preds in baselines.items():
        rmse = np.sqrt(mean_squared_error(y_true, baseline_preds))
        mae = mean_absolute_error(y_true, baseline_preds)
        r2 = r2_score(y_true, baseline_preds)
        
        baseline_performance[name] = {
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'rmse_vs_model': (rmse - model_rmse) / model_rmse * 100,  # % worse than model
            'mae_vs_model': (mae - model_mae) / model_mae * 100
        }
    
    # Summary statistics
    best_baseline_rmse = min(perf['rmse'] for perf in baseline_performance.values())
    best_baseline_name = min(baseline_performance.keys(), 
                           key=lambda x: baseline_performance[x]['rmse'])
    
    model_improvement_vs_best = (best_baseline_rmse - model_rmse) / best_baseline_rmse * 100
    
    return {
        'model_performance': {
            'rmse': model_rmse,
            'mae': model_mae,
            'r2': model_r2
        },
        'baseline_performance': baseline_performance,
        'best_baseline': {
            'name': best_baseline_name,
            'rmse': best_baseline_rmse
        },
        'model_improvement_vs_best_baseline_percent': model_improvement_vs_best,
        'beats_all_baselines': all(perf['rmse'] > model_rmse for perf in baseline_performance.values())
    }


def evaluate_uncertainty_quantification(y_true: np.ndarray, y_pred: np.ndarray,
                                      y_lower: np.ndarray, y_upper: np.ndarray,
                                      confidence_level: float = 0.8) -> Dict[str, any]:
    """Comprehensive evaluation of uncertainty quantification performance.
    
    Args:
        y_true: True target values
        y_pred: Point predictions
        y_lower: Lower bounds of prediction intervals
        y_upper: Upper bounds of prediction intervals
        confidence_level: Target confidence level (e.g., 0.8 for 80%)
        
    Returns:
        Dictionary with uncertainty quantification metrics
    """
    n_samples = len(y_true)
    
    # Coverage metrics
    in_interval = (y_true >= y_lower) & (y_true <= y_upper)
    empirical_coverage = np.mean(in_interval)
    coverage_error = abs(empirical_coverage - confidence_level)
    
    # Interval width metrics
    interval_widths = y_upper - y_lower
    avg_width = np.mean(interval_widths)
    median_width = np.median(interval_widths)
    width_std = np.std(interval_widths)
    
    # Relative width (as percentage of prediction)
    relative_widths = interval_widths / np.maximum(np.abs(y_pred), 1) * 100
    avg_relative_width = np.mean(relative_widths)
    
    # Prediction interval efficiency (coverage per unit width)
    efficiency = empirical_coverage / (avg_width / np.mean(np.abs(y_true))) if avg_width > 0 else 0
    
    # Risk assessment metrics
    overconfident_rate = np.mean(~in_interval)  # Fraction outside intervals
    underconfident_rate = np.mean(interval_widths > 2 * np.abs(y_true - y_pred))
    
    # Business risk metrics (for pricing decisions)
    underestimate_risk = np.sum((y_true > y_upper) * (y_true - y_upper)) / n_samples
    overestimate_risk = np.sum((y_true < y_lower) * (y_lower - y_true)) / n_samples
    
    return {
        'coverage_metrics': {
            'target_coverage': confidence_level,
            'empirical_coverage': empirical_coverage,
            'coverage_error': coverage_error,
            'samples_in_interval': np.sum(in_interval),
            'total_samples': n_samples
        },
        'interval_quality': {
            'avg_width': avg_width,
            'median_width': median_width,
            'width_std': width_std,
            'avg_relative_width_percent': avg_relative_width,
            'interval_efficiency': efficiency
        },
        'risk_assessment': {
            'overconfident_rate': overconfident_rate,
            'underconfident_rate': underconfident_rate,
            'underestimate_risk_dollars': underestimate_risk,
            'overestimate_risk_dollars': overestimate_risk
        },
        'business_interpretation': {
            'coverage_quality': 'Excellent' if coverage_error < 0.05 else 'Good' if coverage_error < 0.1 else 'Poor',
            'interval_usefulness': 'High' if avg_relative_width < 30 else 'Medium' if avg_relative_width < 50 else 'Low',
            'recommended_use_case': _get_uncertainty_use_case_recommendation(empirical_coverage, avg_relative_width)
        }
    }


def _get_uncertainty_use_case_recommendation(coverage: float, relative_width: float) -> str:
    """Recommend use cases based on uncertainty quality."""
    if coverage >= 0.75 and relative_width <= 25:
        return "Production-ready for high-stakes pricing decisions"
    elif coverage >= 0.7 and relative_width <= 35:
        return "Suitable for decision support and risk assessment"
    elif coverage >= 0.6:
        return "Useful for exploratory analysis and trend identification"
    else:
        return "Needs improvement before business deployment"

class ModelEvaluator:
    """Comprehensive model evaluation for business contexts."""
    
    def __init__(self, output_dir: str = "./plots/"):
        """Initialize evaluator with output directory.
        
        Args:
            output_dir: Directory to save plots and results
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Set professional plotting style
        plt.style.use('seaborn-v0_8-whitegrid')
        sns.set_palette("husl")
    
    def compute_prediction_intervals(self, y_true: np.array, y_pred: np.array, 
                                   alpha: float = 0.2) -> Tuple[np.array, np.array]:
        """Compute prediction intervals using residual analysis.
        
        Enhanced from internal/evaluation.py for business uncertainty quantification.
        
        Args:
            y_true: True values (used to compute residuals)
            y_pred: Predicted values
            alpha: Confidence level (0.2 = 80% confidence interval)
            
        Returns:
            Tuple of (lower_bounds, upper_bounds) for prediction intervals
        """
        # Compute residuals
        residuals = y_true - y_pred
        
        # Calculate quantiles for prediction intervals
        lower_q = alpha / 2
        upper_q = 1 - alpha / 2
        
        q_low, q_high = np.quantile(residuals, [lower_q, upper_q])
        
        # Apply residual quantiles to predictions
        y_lower = y_pred + q_low
        y_upper = y_pred + q_high
        
        # Ensure non-negative for price data
        y_lower = np.maximum(y_lower, 0)
        
        return y_lower, y_upper
    
    def evaluate_prediction_intervals(self, y_true: np.array, y_pred: np.array, 
                                    y_lower: np.array, y_upper: np.array) -> Dict[str, float]:
        """Evaluate prediction interval quality.
        
        Args:
            y_true: True values
            y_pred: Predicted values  
            y_lower: Lower bound predictions
            y_upper: Upper bound predictions
            
        Returns:
            Dictionary with interval evaluation metrics
        """
        # Coverage: percentage of true values within intervals
        coverage = np.mean((y_true >= y_lower) & (y_true <= y_upper)) * 100
        
        # Average interval width
        avg_width = np.mean(y_upper - y_lower)
        
        # Relative interval width (as percentage of prediction)
        relative_width = np.mean((y_upper - y_lower) / np.abs(y_pred)) * 100
        
        # Prediction interval efficiency (coverage per unit width)
        efficiency = coverage / avg_width if avg_width > 0 else 0
        
        return {
            'coverage_percent': coverage,
            'avg_interval_width': avg_width,
            'relative_width_percent': relative_width,
            'interval_efficiency': efficiency,
            'pred_in_interval_count': np.sum((y_true >= y_lower) & (y_true <= y_upper)),
            'total_predictions': len(y_true)
        }

    def compute_business_metrics(self, y_true: np.array, y_pred: np.array, 
                                model_name: str = "Model") -> Dict[str, float]:
        """Calculate business-relevant metrics.
        
        Args:
            y_true: True values
            y_pred: Predicted values
            model_name: Name of the model for reporting
            
        Returns:
            Dictionary of business metrics
        """
        # Ensure positive predictions
        y_pred_pos = np.maximum(y_pred, 1)
        y_true = np.maximum(y_true, 1)
        
        # Standard regression metrics
        mae = mean_absolute_error(y_true, y_pred_pos)
        mse = mean_squared_error(y_true, y_pred_pos)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred_pos)
        
        # Business-specific metrics
        mape = np.mean(np.abs((y_true - y_pred_pos) / y_true)) * 100
        
        # Root Mean Squared Logarithmic Error (RMSLE)
        rmsle = np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred_pos)))
        
        # Tolerance-based accuracy (key business metric)
        within_10_pct = np.mean(np.abs(y_true - y_pred_pos) / y_true <= 0.10) * 100
        within_15_pct = np.mean(np.abs(y_true - y_pred_pos) / y_true <= 0.15) * 100
        within_25_pct = np.mean(np.abs(y_true - y_pred_pos) / y_true <= 0.25) * 100
        
        # Error distribution analysis
        errors = y_pred_pos - y_true
        
        return {
            'model_name': model_name,
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'mape': mape,
            'rmsle': rmsle,
            'within_10_pct': within_10_pct,
            'within_15_pct': within_15_pct,
            'within_25_pct': within_25_pct,
            'mean_error': np.mean(errors),
            'median_error': np.median(errors),
            'std_error': np.std(errors),
            'max_underestimate': np.min(errors),
            'max_overestimate': np.max(errors)
        }
    
    def create_performance_plots(self, y_true: np.array, y_pred: np.array, 
                                model_name: str = "Model") -> str:
        """Create comprehensive performance visualization.
        
        Args:
            y_true: True values
            y_pred: Predicted values
            model_name: Name of the model
            
        Returns:
            Path to saved plot
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'{model_name} - Performance Analysis', fontsize=16, fontweight='bold')
        
        # Ensure positive values
        y_pred_pos = np.maximum(y_pred, 1)
        y_true = np.maximum(y_true, 1)
        
        # 1. Actual vs Predicted
        ax1 = axes[0, 0]
        ax1.scatter(y_true, y_pred_pos, alpha=0.6, s=20)
        
        # Perfect prediction line
        min_val = min(y_true.min(), y_pred_pos.min())
        max_val = max(y_true.max(), y_pred_pos.max())
        ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')
        
        # +/-15% tolerance bands
        ax1.plot([min_val, max_val], [min_val*0.85, max_val*0.85], 'g--', alpha=0.7, label='-15% Tolerance')
        ax1.plot([min_val, max_val], [min_val*1.15, max_val*1.15], 'g--', alpha=0.7, label='+15% Tolerance')
        
        ax1.set_xlabel('Actual Price ($)')
        ax1.set_ylabel('Predicted Price ($)')
        ax1.set_title('Actual vs Predicted Prices')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Format axes with currency
        ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
        ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
        
        # 2. Error Distribution
        ax2 = axes[0, 1]
        errors = y_pred_pos - y_true
        relative_errors = errors / y_true * 100
        
        ax2.hist(relative_errors, bins=50, alpha=0.7, edgecolor='black')
        ax2.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')
        ax2.axvline(relative_errors.mean(), color='green', linestyle='--', linewidth=2, 
                   label=f'Mean Error ({relative_errors.mean():.1f}%)')
        
        ax2.set_xlabel('Relative Error (%)')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Error Distribution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Residuals vs Predicted
        ax3 = axes[1, 0]
        ax3.scatter(y_pred_pos, errors, alpha=0.6, s=20)
        ax3.axhline(0, color='red', linestyle='--', linewidth=2)
        
        # Add trend line
        z = np.polyfit(y_pred_pos, errors, 1)
        p = np.poly1d(z)
        ax3.plot(y_pred_pos, p(y_pred_pos), 'orange', linewidth=2, label=f'Trend (slope={z[0]:.2e})')
        
        ax3.set_xlabel('Predicted Price ($)')
        ax3.set_ylabel('Residuals ($)')
        ax3.set_title('Residuals vs Predicted')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
        
        # 4. Tolerance Analysis
        ax4 = axes[1, 1]
        tolerances = [5, 10, 15, 20, 25, 30]
        accuracies = [np.mean(np.abs(relative_errors) <= tol) * 100 for tol in tolerances]
        
        bars = ax4.bar(tolerances, accuracies, alpha=0.7, edgecolor='black')
        ax4.axhline(80, color='red', linestyle='--', linewidth=2, label='Target (80%)')
        
        # Add value labels on bars
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{acc:.1f}%', ha='center', va='bottom')
        
        ax4.set_xlabel('Tolerance (%)')
        ax4.set_ylabel('Accuracy (%)')
        ax4.set_title('Prediction Accuracy by Tolerance')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 100)
        
        plt.tight_layout()
        
        # Save plot
        plot_path = self.output_dir / f'{model_name.lower().replace(" ", "_")}_performance.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return str(plot_path)
    
    def create_feature_importance_plot(self, importance_data: List[Dict], 
                                     model_name: str = "Model") -> str:
        """Create sophisticated feature importance visualization with econometric categorization.
        
        Args:
            importance_data: List of dictionaries with 'feature' and 'importance'
            model_name: Name of the model
            
        Returns:
            Path to saved plot
        """
        if not importance_data:
            return "No feature importance data available"
        
        # Convert to DataFrame for easier plotting
        df_importance = pd.DataFrame(importance_data)
        
        # Create advanced visualization with two subplots
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
        fig.suptitle(f'{model_name} - Advanced Feature Importance Analysis', fontsize=16, fontweight='bold')
        
        # ==================== LEFT PLOT: Traditional Importance ====================
        
        # Horizontal bar plot for top features
        bars1 = ax1.barh(range(len(df_importance)), df_importance['importance'], 
                        alpha=0.8, edgecolor='black')
        
        # Customize first plot
        ax1.set_yticks(range(len(df_importance)))
        ax1.set_yticklabels(df_importance['feature'])
        ax1.set_xlabel('Feature Importance')
        ax1.set_title('Top Features by Importance')
        ax1.grid(True, alpha=0.3, axis='x')
        
        # Add value labels
        for i, (bar, importance) in enumerate(zip(bars1, df_importance['importance'])):
            ax1.text(importance + importance*0.01, bar.get_y() + bar.get_height()/2, 
                    f'{importance:.3f}', va='center', ha='left', fontsize=8)
        
        # Color bars by importance
        colors1 = plt.cm.viridis(np.linspace(0, 1, len(bars1)))
        for bar, color in zip(bars1, colors1):
            bar.set_color(color)
        
        # ==================== RIGHT PLOT: Econometric Categories ====================
        
        # Categorize features by econometric type
        all_features = df_importance['feature'].tolist()
        feature_categories = self._categorize_econometric_features(all_features)
        
        # Calculate category-wise importance
        category_importance = {}
        category_counts = {}
        
        for category, features in feature_categories.items():
            if features:  # Only process non-empty categories
                category_data = df_importance[df_importance['feature'].isin(features)]
                category_importance[category] = category_data['importance'].sum()
                category_counts[category] = len(features)
        
        # Create category plot
        if category_importance:
            categories = list(category_importance.keys())
            importances = list(category_importance.values())
            counts = [category_counts[cat] for cat in categories]
            
            # Create color map for categories
            category_colors = {
                'depreciation': '#FF6B6B',    # Red
                'seasonality': '#4ECDC4',     # Teal  
                'interactions': '#45B7D1',    # Blue
                'binning': '#96CEB4',         # Green
                'normalization': '#FFEAA7',   # Yellow
                'data_quality': '#DDA0DD',    # Plum
                'basic': '#95A5A6'            # Gray
            }
            
            colors2 = [category_colors.get(cat, '#95A5A6') for cat in categories]
            
            bars2 = ax2.bar(categories, importances, alpha=0.8, color=colors2, edgecolor='black')
            
            ax2.set_xlabel('Feature Categories')
            ax2.set_ylabel('Cumulative Importance')
            ax2.set_title('Econometric Feature Categories')
            ax2.grid(True, alpha=0.3, axis='y')
            
            # Rotate x-axis labels for better readability
            plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
            
            # Add importance and count labels on bars
            for bar, importance, count in zip(bars2, importances, counts):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{importance:.3f}\n({count} features)', ha='center', va='bottom', fontsize=8)
        else:
            ax2.text(0.5, 0.5, 'No econometric features detected', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)
            ax2.set_title('Econometric Feature Categories')
        
        plt.tight_layout()
        
        # Save plot
        plot_path = self.output_dir / f'{model_name.lower().replace(" ", "_")}_feature_importance_advanced.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return str(plot_path)
    
    def _categorize_econometric_features(self, feature_list: List[str]) -> Dict[str, List[str]]:
        """Categorize features by econometric type for analysis.
        
        Args:
            feature_list: List of feature names to categorize
            
        Returns:
            Dictionary mapping categories to lists of feature names
        """
        categories = {
            'depreciation': [],
            'seasonality': [],
            'interactions': [],
            'binning': [],
            'normalization': [],
            'data_quality': [],
            'basic': []
        }
        
        for feature in feature_list:
            categorized = False
            
            # Depreciation curves
            if any(pattern in feature for pattern in ['age_squared', 'log1p_age']):
                categories['depreciation'].append(feature)
                categorized = True
            
            # Seasonality and time trends
            elif any(pattern in feature for pattern in ['_sin', '_cos', '_trend', '2008_2009']):
                categories['seasonality'].append(feature)
                categorized = True
            
            # Interaction features
            elif any(pattern in feature for pattern in ['_interaction', '_x_']):
                categories['interactions'].append(feature)
                categorized = True
            
            # Binning features
            elif feature.endswith('_bucket'):
                categories['binning'].append(feature)
                categorized = True
            
            # Group normalization (z-scores)
            elif '_z_by_' in feature:
                categories['normalization'].append(feature)
                categorized = True
            
            # Data quality features
            elif any(pattern in feature for pattern in ['_na', 'completeness']):
                categories['data_quality'].append(feature)
                categorized = True
            
            # Everything else is basic
            if not categorized:
                categories['basic'].append(feature)
        
        return categories
    
    def create_model_comparison_plot(self, comparison_df: pd.DataFrame) -> str:
        """Create model comparison visualization.
        
        Args:
            comparison_df: DataFrame with model comparison metrics
            
        Returns:
            Path to saved plot
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Model Comparison - Key Metrics', fontsize=16, fontweight='bold')
        
        # Key metrics to compare
        metrics = [
            ('rmse', 'RMSE ($)', 'Lower is Better'),
            ('within_15_pct', 'Within 15% Accuracy (%)', 'Higher is Better'),
            ('mape', 'MAPE (%)', 'Lower is Better'),
            ('rmsle', 'RMSLE', 'Lower is Better')
        ]
        
        for i, (metric, title, note) in enumerate(metrics):
            ax = axes[i//2, i%2]
            
            bars = ax.bar(comparison_df['model'], comparison_df[metric], 
                         alpha=0.8, edgecolor='black')
            
            ax.set_title(f'{title}\n({note})')
            ax.set_ylabel(title)
            ax.grid(True, alpha=0.3, axis='y')
            
            # Rotate x-axis labels if needed
            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
            
            # Add value labels on bars
            for bar, value in zip(bars, comparison_df[metric]):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                       f'{value:.1f}', ha='center', va='bottom')
            
            # Color bars (best performance in green)
            if 'higher is better' in note.lower():
                best_idx = comparison_df[metric].idxmax()
            else:
                best_idx = comparison_df[metric].idxmin()
            
            colors = ['lightcoral' if i != best_idx else 'lightgreen' 
                     for i in range(len(bars))]
            for bar, color in zip(bars, colors):
                bar.set_color(color)
        
        plt.tight_layout()
        
        # Save plot
        plot_path = self.output_dir / 'model_comparison.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return str(plot_path)
    
    def create_baseline_comparison_plot(self, baseline_results: Dict[str, any]) -> str:
        """Create visualization comparing model performance against baselines.
        
        Args:
            baseline_results: Results from evaluate_against_baselines()
            
        Returns:
            Path to saved plot
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle('Model vs. Sophisticated Baselines', fontsize=16, fontweight='bold')
        
        # Prepare data
        baseline_names = list(baseline_results['baseline_performance'].keys())
        baseline_rmse = [baseline_results['baseline_performance'][name]['rmse'] 
                        for name in baseline_names]
        model_rmse = baseline_results['model_performance']['rmse']
        
        # Left plot: RMSE comparison
        colors = ['lightcoral' if rmse > model_rmse else 'lightblue' for rmse in baseline_rmse]
        bars1 = ax1.bar(baseline_names + ['ML Model'], baseline_rmse + [model_rmse], 
                        color=colors + ['lightgreen'], alpha=0.8, edgecolor='black')
        
        ax1.set_title('RMSE Comparison (Lower is Better)')
        ax1.set_ylabel('RMSE ($)')
        ax1.grid(True, alpha=0.3, axis='y')
        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
        
        # Add value labels
        for bar, value in zip(bars1, baseline_rmse + [model_rmse]):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'${value:,.0f}', ha='center', va='bottom', fontsize=9)
        
        # Right plot: Improvement percentages
        improvements = [baseline_results['baseline_performance'][name]['rmse_vs_model'] 
                       for name in baseline_names]
        
        colors2 = ['green' if imp > 0 else 'red' for imp in improvements]
        bars2 = ax2.bar(baseline_names, improvements, color=colors2, alpha=0.7, edgecolor='black')
        
        ax2.set_title('ML Model Improvement vs. Baselines')
        ax2.set_ylabel('Baseline RMSE vs. Model (%)')
        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.7)
        ax2.grid(True, alpha=0.3, axis='y')
        plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
        
        # Add value labels
        for bar, imp in zip(bars2, improvements):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., 
                    height + (height*0.05 if height > 0 else height*0.05),
                    f'{imp:+.1f}%', ha='center', va='bottom' if height > 0 else 'top', fontsize=9)
        
        plt.tight_layout()
        
        # Save plot
        plot_path = self.output_dir / 'baseline_comparison.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return str(plot_path)
    
    def create_uncertainty_analysis_plot(self, y_true: np.ndarray, y_pred: np.ndarray,
                                       y_lower: np.ndarray, y_upper: np.ndarray,
                                       model_name: str = "Model", sample_size: int = 1000) -> str:
        """Create comprehensive uncertainty quantification visualization.
        
        Args:
            y_true: True values
            y_pred: Predicted values
            y_lower: Lower prediction bounds
            y_upper: Upper prediction bounds
            model_name: Model name for title
            sample_size: Number of points to show (for readability)
            
        Returns:
            Path to saved plot
        """
        # Sample for visualization
        if len(y_true) > sample_size:
            indices = np.random.choice(len(y_true), sample_size, replace=False)
            y_true_sample = y_true[indices]
            y_pred_sample = y_pred[indices]
            y_lower_sample = y_lower[indices]
            y_upper_sample = y_upper[indices]
        else:
            y_true_sample = y_true
            y_pred_sample = y_pred
            y_lower_sample = y_lower
            y_upper_sample = y_upper
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'{model_name} - Uncertainty Quantification Analysis', fontsize=16, fontweight='bold')
        
        # 1. Prediction intervals scatter plot
        ax1 = axes[0, 0]
        
        # Sort by predicted value for better visualization
        sort_idx = np.argsort(y_pred_sample)
        y_true_sorted = y_true_sample[sort_idx]
        y_pred_sorted = y_pred_sample[sort_idx]
        y_lower_sorted = y_lower_sample[sort_idx]
        y_upper_sorted = y_upper_sample[sort_idx]
        
        x_axis = np.arange(len(y_pred_sorted))
        
        # Plot prediction intervals
        ax1.fill_between(x_axis, y_lower_sorted, y_upper_sorted, alpha=0.3, color='lightblue', label='Prediction Interval')
        ax1.plot(x_axis, y_pred_sorted, 'b-', alpha=0.7, label='Predictions')
        ax1.scatter(x_axis, y_true_sorted, alpha=0.6, s=10, color='red', label='Actual Values')
        
        ax1.set_xlabel('Sample Index (sorted by prediction)')
        ax1.set_ylabel('Price ($)')
        ax1.set_title('Prediction Intervals vs. Actual Values')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Coverage analysis
        ax2 = axes[0, 1]
        in_interval = (y_true >= y_lower) & (y_true <= y_upper)
        coverage_by_quantile = []
        quantiles = np.linspace(0, 1, 21)  # 20 quantiles
        
        for i in range(len(quantiles)-1):
            q_low, q_high = np.quantile(y_pred, [quantiles[i], quantiles[i+1]])
            mask = (y_pred >= q_low) & (y_pred <= q_high)
            if np.sum(mask) > 0:
                coverage_by_quantile.append(np.mean(in_interval[mask]))
            else:
                coverage_by_quantile.append(0)
        
        ax2.plot(quantiles[:-1], coverage_by_quantile, 'o-', linewidth=2, markersize=6)
        ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Target Coverage (80%)')
        ax2.set_xlabel('Prediction Quantile')
        ax2.set_ylabel('Empirical Coverage')
        ax2.set_title('Coverage by Prediction Quantile')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Interval width distribution
        ax3 = axes[1, 0]
        interval_widths = y_upper - y_lower
        ax3.hist(interval_widths, bins=50, alpha=0.7, edgecolor='black')
        ax3.axvline(np.median(interval_widths), color='red', linestyle='--', linewidth=2, 
                   label=f'Median Width: ${np.median(interval_widths):,.0f}')
        ax3.set_xlabel('Interval Width ($)')
        ax3.set_ylabel('Frequency')
        ax3.set_title('Prediction Interval Width Distribution')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. Calibration plot
        ax4 = axes[1, 1]
        # Create calibration curve
        n_bins = 10
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        empirical_coverage_bins = []
        expected_coverage_bins = []
        
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            # Find predictions in this confidence range
            confidence_scores = 1 - 2 * np.minimum(
                np.abs(y_pred - y_lower) / np.maximum(y_upper - y_lower, 1e-6),
                np.abs(y_pred - y_upper) / np.maximum(y_upper - y_lower, 1e-6)
            )
            
            in_bin = (confidence_scores >= bin_lower) & (confidence_scores < bin_upper)
            if np.sum(in_bin) > 0:
                empirical_coverage_bins.append(np.mean(in_interval[in_bin]))
                expected_coverage_bins.append((bin_lower + bin_upper) / 2)
            
        if empirical_coverage_bins:
            ax4.plot(expected_coverage_bins, empirical_coverage_bins, 'o-', linewidth=2, markersize=8)
            ax4.plot([0, 1], [0, 1], 'r--', alpha=0.7, label='Perfect Calibration')
            ax4.set_xlabel('Expected Coverage')
            ax4.set_ylabel('Empirical Coverage')
            ax4.set_title('Calibration Curve')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, 'Insufficient data\nfor calibration plot', 
                    ha='center', va='center', transform=ax4.transAxes)
        
        plt.tight_layout()
        
        # Save plot
        plot_path = self.output_dir / f'{model_name.lower().replace(" ", "_")}_uncertainty_analysis.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return str(plot_path)
    
    def evaluate_econometric_impact(self, training_results: Dict[str, any]) -> Dict[str, any]:
        """Evaluate the business impact of econometric feature engineering.
        
        This method provides sophisticated analysis of how advanced features
        contribute to model performance and business value.
        
        Args:
            training_results: Results from model training with feature importance
            
        Returns:
            Dictionary with econometric impact analysis
        """
        if 'econometric_analysis' not in training_results:
            return {'error': 'No econometric analysis available in training results'}
        
        econ_analysis = training_results['econometric_analysis']
        validation_metrics = training_results.get('validation_metrics', {})
        
        # Calculate sophistication score
        sophistication_score = self._calculate_sophistication_score(econ_analysis)
        
        # Business impact assessment
        business_impact = self._assess_business_impact(validation_metrics, econ_analysis)
        
        # Generate feature engineering recommendations
        recommendations = self._generate_feature_recommendations(econ_analysis)
        
        return {
            'sophistication_metrics': {
                'overall_score': sophistication_score,
                'econometric_feature_share': econ_analysis['overall_contribution']['econometric_importance_share'],
                'categories_utilized': econ_analysis['sophistication_metrics']['categories_present'],
                'feature_density': econ_analysis['sophistication_metrics']['advanced_feature_density']
            },
            'business_impact': business_impact,
            'recommendations': recommendations,
            'category_performance': econ_analysis['category_performance']
        }
    
    def _calculate_sophistication_score(self, econ_analysis: Dict) -> float:
        """Calculate an overall sophistication score (0-100) based on feature usage."""
        
        # Base components for sophistication
        category_score = (econ_analysis['sophistication_metrics']['categories_present'] / 6) * 30  # Max 30 points
        density_score = min(econ_analysis['sophistication_metrics']['advanced_feature_density'] / 50, 1) * 25  # Max 25 points
        importance_score = (econ_analysis['overall_contribution']['econometric_importance_share'] / 100) * 25  # Max 25 points
        
        # Bonus for having top econometric features
        top_features_bonus = min(len(econ_analysis['overall_contribution']['top_econometric_features']), 5) * 4  # Max 20 points
        
        total_score = category_score + density_score + importance_score + top_features_bonus
        return min(total_score, 100.0)  # Cap at 100
    
    def _assess_business_impact(self, metrics: Dict, econ_analysis: Dict) -> Dict[str, any]:
        """Assess the business value created by econometric features."""
        
        # Get baseline business metrics
        rmse = metrics.get('rmse', 0)
        within_15_pct = metrics.get('within_15_pct', 0)
        r2 = metrics.get('r2', 0)
        
        # Estimate econometric contribution to performance
        econ_share = econ_analysis['overall_contribution']['econometric_importance_share']
        
        # Business value calculations (estimated impact)
        estimated_rmse_improvement = (econ_share / 100) * 0.15  # 15% max improvement potential
        estimated_accuracy_improvement = (econ_share / 100) * 10  # 10 percentage points max
        
        return {
            'current_performance': {
                'rmse_dollars': rmse,
                'accuracy_within_15pct': within_15_pct,
                'r_squared': r2
            },
            'estimated_econometric_value': {
                'rmse_improvement_potential': f"{estimated_rmse_improvement:.1%}",
                'accuracy_improvement_potential': f"{estimated_accuracy_improvement:.1f} percentage points",
                'model_sophistication': "PhD-level econometric modeling" if econ_share > 30 else "Advanced statistical modeling"
            },
            'business_benefits': [
                "Non-linear depreciation modeling captures real-world asset behavior",
                "Seasonal adjustments improve timing-based predictions", 
                "Market crisis indicators handle economic volatility",
                "Group normalization enables fair cross-category comparisons",
                "Data quality features extract value from missing information patterns"
            ]
        }
    
    def _generate_feature_recommendations(self, econ_analysis: Dict) -> List[str]:
        """Generate actionable recommendations for feature engineering improvements."""
        
        recommendations = []
        category_perf = econ_analysis['category_performance']
        econ_share = econ_analysis['overall_contribution']['econometric_importance_share']
        
        # General sophistication assessment
        if econ_share < 20:
            recommendations.append("[UP] Increase econometric feature contribution (currently low)")
        elif econ_share > 60:
            recommendations.append("[FAST] Excellent econometric feature utilization!")
        
        # Category-specific recommendations
        if 'depreciation' not in category_perf or category_perf.get('depreciation', {}).get('count', 0) == 0:
            recommendations.append("[DOWN] Add depreciation curve features (age_squared, log1p_age)")
        
        if 'seasonality' not in category_perf or category_perf.get('seasonality', {}).get('count', 0) < 2:
            recommendations.append("[DATE] Enhance seasonal modeling (sin/cos transformations, crisis indicators)")
        
        if 'interactions' not in category_perf or category_perf.get('interactions', {}).get('count', 0) == 0:
            recommendations.append("[REFRESH] Add interaction features (usagexage, horsepowerxage)")
        
        if 'normalization' not in category_perf or category_perf.get('normalization', {}).get('count', 0) == 0:
            recommendations.append("[DATA] Implement group normalization (z-scores by equipment type)")
        
        if 'data_quality' not in category_perf or category_perf.get('data_quality', {}).get('count', 0) < 3:
            recommendations.append("[SEARCH] Expand data quality features (missingness indicators, completeness scores)")
        
        # Advanced recommendations
        recommendations.extend([
            "[TARGET] Consider ensemble methods to leverage econometric feature diversity",
            "[EVAL] Monitor feature importance changes over time for model stability",
            "ðŸ§ª Experiment with polynomial interactions for non-linear relationships"
        ])
        
        return recommendations[:8]  # Limit to most important recommendations


def evaluate_model_comprehensive(y_true: np.array, y_pred: np.array, 
                                model_name: str = "Model",
                                feature_importance: Optional[List[Dict]] = None,
                                output_dir: str = "./plots/",
                                include_intervals: bool = True) -> Dict[str, any]:
    """Comprehensive model evaluation with all visualizations and metrics.
    
    Enhanced with prediction intervals from internal/evaluation.py
    
    Args:
        y_true: True values
        y_pred: Predicted values
        model_name: Name of the model
        feature_importance: Optional feature importance data
        output_dir: Output directory for plots
        include_intervals: Whether to compute prediction intervals
        
    Returns:
        Dictionary with all evaluation results including prediction intervals
    """
    evaluator = ModelEvaluator(output_dir)
    
    # Compute standard metrics
    metrics = evaluator.compute_business_metrics(y_true, y_pred, model_name)
    
    # Compute prediction intervals (enhanced from internal/)
    prediction_intervals = None
    interval_metrics = None
    if include_intervals:
        y_lower, y_upper = evaluator.compute_prediction_intervals(y_true, y_pred, alpha=0.2)
        interval_metrics = evaluator.evaluate_prediction_intervals(y_true, y_pred, y_lower, y_upper)
        prediction_intervals = {
            'lower_bounds': y_lower,
            'upper_bounds': y_upper,
            'coverage_percent': interval_metrics['coverage_percent'],
            'avg_width': interval_metrics['avg_interval_width']
        }
    
    # Create visualizations
    performance_plot = evaluator.create_performance_plots(y_true, y_pred, model_name)
    
    # Feature importance plot (if available)
    feature_plot = None
    if feature_importance:
        feature_plot = evaluator.create_feature_importance_plot(feature_importance, model_name)
    
    # Enhanced report with prediction intervals
    print(f"\n{'='*60}")
    print(f"EVALUATION REPORT: {model_name.upper()}")
    print(f"{'='*60}")
    print(f"RMSE: ${metrics['rmse']:,.0f}")
    print(f"Within 15% Accuracy: {metrics['within_15_pct']:.1f}%")
    print(f"RÂ² Score: {metrics['r2']:.3f}")
    print(f"MAPE: {metrics['mape']:.1f}%")
    
    if prediction_intervals:
        print(f"\n[DATA] PREDICTION INTERVALS (80% Confidence)")
        print(f"Coverage: {interval_metrics['coverage_percent']:.1f}%")
        print(f"Average Width: ${interval_metrics['avg_interval_width']:,.0f}")
        print(f"Relative Width: {interval_metrics['relative_width_percent']:.1f}%")
    
    print(f"{'='*60}")
    
    return {
        'metrics': metrics,
        'prediction_intervals': prediction_intervals,
        'interval_metrics': interval_metrics,
        'performance_plot': performance_plot,
        'feature_importance_plot': feature_plot
    }


if __name__ == "__main__":
    # Test the evaluation module
    print("âœ“ Evaluation module loaded successfully")

def plot_actual_vs_pred(y_true: np.ndarray,
                        preds: Dict[str, np.ndarray],
                        out_path: str,
                        sample: int = 2000) -> str:
    """Scatter plot of actual vs predicted for multiple models.

    Ports the simple multi-model comparator from the prototype pipeline,
    preserving behavior while standardizing on the current output locations.

    Args:
        y_true: Array of true target values
        preds: Mapping of model name to predicted values array
        out_path: File path to save the figure
        sample: Optional sub-sample size for readability

    Returns:
        The output path the figure was saved to
    """
    n = len(y_true)
    idx = np.arange(n)
    if n > sample:
        rng = np.random.default_rng(42)
        idx = rng.choice(idx, size=sample, replace=False)

    plt.figure(figsize=(7, 6))
    max_val = float(np.percentile(y_true, 99))
    for name, p in preds.items():
        plt.scatter(y_true[idx], p[idx], s=10, alpha=0.6, label=name)
    plt.plot([0, max_val], [0, max_val], "k--", lw=1, label="ideal")
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.title("Model Performance: Actual vs Predicted")
    plt.legend()
    plt.tight_layout()
    Path(out_path).parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=150)
    plt.close()
    return out_path
